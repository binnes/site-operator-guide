{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"apply/","text":"Apply \u00b6 After you've gotten a basic understanding of how things work by going through the labs in the Learn section, Apply is here with more detailed information on how to use these assets on an actual project.","title":"Overview"},{"location":"apply/#apply","text":"After you've gotten a basic understanding of how things work by going through the labs in the Learn section, Apply is here with more detailed information on how to use these assets on an actual project.","title":"Apply"},{"location":"apply/bill-of-material-reference/","text":"Bill of Material reference \u00b6 The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/ BOM metadata \u00b6 The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform BOM spec \u00b6 The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations BOM module definition \u00b6 A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules. BOM Module dependencies If the module depends on other modules, the relationships can be defined in the dependencies block. However, in most cases it is not necessary to explicitly define the dependencies. Through the module metadata, the iascable tool knows the required dependencies for each module and can \"auto-wire\" the modules together. If necessary, iascable will automatically add modules to the BOM if they are required to satisfy a required module dependency. If there are multiple instances of a dependent module defined in the BOM then iascable will \"auto-wire\" the dependency to the \"default\" dependent module. The \"default\" dependent module is the one that uses the default alias name OR has the default: true attribute added to it. If a default cannot be identified then ANOTHER instance of the module will be automatically added to the BOM. If this behavior is not desired then the desired dependent module can be referenced in the dependencies block. For example: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp The ibm-vpc-subnets module depends on ibm-vpc . An explicit declaration of the dependency is not required here though because the ibm-vpc module is the default instance and all of the ibm-vpc-subnets are auto-wired to that instance. (In fact the ibm-vpc module doesn't even need to be explicitly listed in the BOM in this case, but it is added for completeness.) The ibm-vpc-ocp module depends on ibm-vpc-subnets to identify where the cluster should be deployed. In this configuration, a default ibm-vpc-subnets instance has not been defined. As a result, iascable will automatically pull in 4th ibm-vpc-subnets instance to satisfy the dependency. This is probably not the desired result and we will want to explicitly define the dependency in the BOM. The updated BOM would look like the following: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp dependencies : - id : subnets ref : cluster_subnets The subnets identifier in the dependencies array refers to the dependency identifier in the module metadata for the ibm-vpc-ocp module. The cluster_subnets value refers to the alias of the target ibm-vpc-subnets module instance. Note: The only exception to iascable automatically pulling dependent modules into the BOM is if there are multiple module options that satisfy the dependency. In this case one of the modules that satisfies dependency must be explicitly added to the BOM. Otherwise the iascable build command will give an error that the dependency cannot be resolved. BOM Module variables The Bill of Materials also allows the module variables to be configured in a variables block. The variables block is an array of variable definitions. At a minimum the variable name must be provided. The available variable names are defined in the module metadata. For each variable, the following values can be provided: Field Description value The default value of the variable. This value will override the default in the module. scope The scope of the variable that defines how the variable will be handled in the global variable namespace. Allowed values are global or module . If the value is global the variable will be added as-is to the global namespace. If the value is module then the variable name will be prefixed with the module alias (e.g. the flavor variable in the cluster module would be named cluster_flavor with module scope and flavor with global scope). alias The alias name that should be given to the variable in the global variable namespace. This alias works in conjunction with the scope value. For example, if the name variable is set to global scope and alias of my_name then a variable named my_name will be added to the global variable namespace and the generated module terraform will map the my_name global variable to the name module variable ( name = var.my_name ) important Flag that indicates the variable should be presented to the user in the generated *.auto.tfvars file even though it has a default value. By default, only required fields (i.e. fields that don't have a default value) are presented to the user. Selectively, other variables can be exposed using this flag for significant configuration values. The objective is to balance flexibility of configuration options with the simplicity of a small number of required inputs Note: The module metadata defines how the outputs from the dependent modules should be wired into a module's input variables. It is not necessary to define any of the \"wired\" variables in the BOM. Example Bill of Material \u00b6 apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Bill of Material"},{"location":"apply/bill-of-material-reference/#bill-of-material-reference","text":"The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/","title":"Bill of Material reference"},{"location":"apply/bill-of-material-reference/#bom-metadata","text":"The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform","title":"BOM metadata"},{"location":"apply/bill-of-material-reference/#bom-spec","text":"The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations","title":"BOM spec"},{"location":"apply/bill-of-material-reference/#bom-module-definition","text":"A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules.","title":"BOM module definition"},{"location":"apply/bill-of-material-reference/#example-bill-of-material","text":"apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Example Bill of Material"},{"location":"apply/end-to-end-testing/","text":"End To End Testing for any Software Module \u00b6 The following provides step-by-step instructions for the end-to-end testing of software BOMs (e.g. Turbonomic) which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic Follow the steps to implement the end-to-end testing \u00b6 Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Clone the Git repo for the software (such as turbonomic) which needs to be tested end-to-end git clone https://github.com/IBM/automation-turbonomic.git Note Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. From the command-line, change directory to the the automation-solutions repository. Navigate to the folder containing the layers that will be generated. For Turbonomic the path is boms/software/turbonomic . Run the generate script to create the automation output. ./generate.sh The output will look something like the following: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200 -openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250 -turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note Every software layer which requires common layer such as gitops or storage as well as configuration will have a symbolic link to the file(s) in the shared location. Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. strategy : matrix : flavor : - ibm storage : - odf - portworx Add environment variables needed for this module in the verify-pr.yaml env : HOME : \"\" IBMCLOUD_API_KEY : \"\" The steps section represents a sequence of tasks that will be executed as part of job. Add the steps which needs to be executed in the sequence. Modify the 200-openshift-gitops BOM to support Gitea. (If you are using the shared gitops BOM then this step isn't necessary.) Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username } Copy the .mocks folder which has the configuration for BOM layer dependency. If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph Trigger the module build which will kick off the end-to-end test for the software to be tested. You can watch the progress from the GitHub Actions tab.","title":"End-To-End Testing"},{"location":"apply/end-to-end-testing/#end-to-end-testing-for-any-software-module","text":"The following provides step-by-step instructions for the end-to-end testing of software BOMs (e.g. Turbonomic) which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic","title":"End To End Testing for any Software Module"},{"location":"apply/end-to-end-testing/#follow-the-steps-to-implement-the-end-to-end-testing","text":"Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Clone the Git repo for the software (such as turbonomic) which needs to be tested end-to-end git clone https://github.com/IBM/automation-turbonomic.git Note Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. From the command-line, change directory to the the automation-solutions repository. Navigate to the folder containing the layers that will be generated. For Turbonomic the path is boms/software/turbonomic . Run the generate script to create the automation output. ./generate.sh The output will look something like the following: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200 -openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250 -turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note Every software layer which requires common layer such as gitops or storage as well as configuration will have a symbolic link to the file(s) in the shared location. Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. strategy : matrix : flavor : - ibm storage : - odf - portworx Add environment variables needed for this module in the verify-pr.yaml env : HOME : \"\" IBMCLOUD_API_KEY : \"\" The steps section represents a sequence of tasks that will be executed as part of job. Add the steps which needs to be executed in the sequence. Modify the 200-openshift-gitops BOM to support Gitea. (If you are using the shared gitops BOM then this step isn't necessary.) Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username } Copy the .mocks folder which has the configuration for BOM layer dependency. If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph Trigger the module build which will kick off the end-to-end test for the software to be tested. You can watch the progress from the GitHub Actions tab.","title":"Follow the steps to implement the end-to-end testing"},{"location":"apply/module-metadata-reference/","text":"Module Metadata reference \u00b6 Coming soon","title":"Module Metadata reference"},{"location":"apply/module-metadata-reference/#module-metadata-reference","text":"Coming soon","title":"Module Metadata reference"},{"location":"apply/multi-catalog/","text":"","title":"Overview"},{"location":"apply/multi-catalog/ascent/","text":"Custom catalogs in ASCENT \u00b6 Coming soon","title":"Custom catalogs in ASCENT"},{"location":"apply/multi-catalog/ascent/#custom-catalogs-in-ascent","text":"Coming soon","title":"Custom catalogs in ASCENT"},{"location":"apply/multi-catalog/custom-catalog/","text":"Creating a Custom Catalog \u00b6 Coming soon","title":"Creating a catalog"},{"location":"apply/multi-catalog/custom-catalog/#creating-a-custom-catalog","text":"Coming soon","title":"Creating a Custom Catalog"},{"location":"apply/multi-catalog/iascable/","text":"Multiple catalogs with IasCable \u00b6 A core set of automation modules has been provided in the module catalog that list public modules from a number of different sources. However, there are situations where additional catalogs are needed (e.g. private modules). With IasCable it is possible to define a Bill of Material with modules from multiple catalogs and generate the automation appropriately. There are two ways to specify the catalog(s) that should be used by IasCable: Annotation in the Bill of Material Argument passed on the command line Note This information applies to IasCable v2.17.0 or higher Bill of Material annotation \u00b6 The Bill of Material provides an annotation area within the \"metadata\" section. The annotations are used to provide additional information for processing Bill of Material. (The full list of annotations and labels can be found in the Bill of Material reference .) In this case, an optional catalogUrls annotation has been provided to define the list of catalogs in which BOM modules can be found. The value is a comma-separated list of urls (either file:// or http(s)://). apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : my-custom-modules annotations : path : test catalogUrls : https://mymodules.someproject.com/index.yaml,https://othermodules.someproject.com/index.yaml spec : modules : - name : custom-module-1 - name : custom-module-2 Catalog URL argument \u00b6 The iascable build command provides a -c argument to pass in the catalog url(s). This argument can be provided multiple times to apply multiple catalogs, with subsequent catalogs taking precedence over previous ones. If not provided, the build uses the main catalog by default - https://modules.cloudnativetoolkit.dev/index.yaml . Note Only the catalog urls provided will be used. If you would like to use a custom catalog in addition to the main catalog then the main catalog will need to be included via the -c argument. iascable build -c https://modules.cloudnativetoolkit.dev/index.yaml -c https://mymodules.someproject.com/index.yaml -i my-custom-modules.yaml","title":"Generating automation"},{"location":"apply/multi-catalog/iascable/#multiple-catalogs-with-iascable","text":"A core set of automation modules has been provided in the module catalog that list public modules from a number of different sources. However, there are situations where additional catalogs are needed (e.g. private modules). With IasCable it is possible to define a Bill of Material with modules from multiple catalogs and generate the automation appropriately. There are two ways to specify the catalog(s) that should be used by IasCable: Annotation in the Bill of Material Argument passed on the command line Note This information applies to IasCable v2.17.0 or higher","title":"Multiple catalogs with IasCable"},{"location":"apply/multi-catalog/iascable/#bill-of-material-annotation","text":"The Bill of Material provides an annotation area within the \"metadata\" section. The annotations are used to provide additional information for processing Bill of Material. (The full list of annotations and labels can be found in the Bill of Material reference .) In this case, an optional catalogUrls annotation has been provided to define the list of catalogs in which BOM modules can be found. The value is a comma-separated list of urls (either file:// or http(s)://). apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : my-custom-modules annotations : path : test catalogUrls : https://mymodules.someproject.com/index.yaml,https://othermodules.someproject.com/index.yaml spec : modules : - name : custom-module-1 - name : custom-module-2","title":"Bill of Material annotation"},{"location":"apply/multi-catalog/iascable/#catalog-url-argument","text":"The iascable build command provides a -c argument to pass in the catalog url(s). This argument can be provided multiple times to apply multiple catalogs, with subsequent catalogs taking precedence over previous ones. If not provided, the build uses the main catalog by default - https://modules.cloudnativetoolkit.dev/index.yaml . Note Only the catalog urls provided will be used. If you would like to use a custom catalog in addition to the main catalog then the main catalog will need to be included via the -c argument. iascable build -c https://modules.cloudnativetoolkit.dev/index.yaml -c https://mymodules.someproject.com/index.yaml -i my-custom-modules.yaml","title":"Catalog URL argument"},{"location":"concepts/overview/","text":"Concepts \u00b6 Todo Complete this section. Topics should include: Overview Cloud environments Infrastructure as code Modular and layered composition Hybrid platform (OpenShift and Kubernetes) Operators Solution lifecycle CI/CD GitOps (including app of apps pattern) Helm (including chart dependencies) Toolkit concepts Modules Bill of Materials (BOM) dependencies configuration / variables In the simplest terms, the Toolkit delivers a set of packaged blocks of automation, called modules, that can be composed into a template, called a Bill of Materials, that describes a desired computing environment. The Bill of Materials can then be built and deployed to generate and run the automation code to create the desired computing environment. This section will explore some of the concepts behind how the toolkit works and the environments generated by some of the modules in the public modules catalog .","title":"Overview"},{"location":"concepts/overview/#concepts","text":"Todo Complete this section. Topics should include: Overview Cloud environments Infrastructure as code Modular and layered composition Hybrid platform (OpenShift and Kubernetes) Operators Solution lifecycle CI/CD GitOps (including app of apps pattern) Helm (including chart dependencies) Toolkit concepts Modules Bill of Materials (BOM) dependencies configuration / variables In the simplest terms, the Toolkit delivers a set of packaged blocks of automation, called modules, that can be composed into a template, called a Bill of Materials, that describes a desired computing environment. The Bill of Materials can then be built and deployed to generate and run the automation code to create the desired computing environment. This section will explore some of the concepts behind how the toolkit works and the environments generated by some of the modules in the public modules catalog .","title":"Concepts"},{"location":"contribute/overview/","text":"Contribute \u00b6 Todo Complete this section Community guidelines for contributing to the project Overview of process of creating a new module (actual instructions should be tasks) request a new public module develop and test a module locally host a private module catalog github automation used to build, test and publish a new module","title":"Overview"},{"location":"contribute/overview/#contribute","text":"Todo Complete this section Community guidelines for contributing to the project Overview of process of creating a new module (actual instructions should be tasks) request a new public module develop and test a module locally host a private module catalog github automation used to build, test and publish a new module","title":"Contribute"},{"location":"getting-started/","text":"Getting started \u00b6 The TechZone Automation provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Learn section. From there, the Apply section provides more detailed guides to move from \"Hello World\" to real world. Along the way, there are a number of Resources available to support and accelerate your journey. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"The TechZone Automation provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Learn section. From there, the Apply section provides more detailed guides to move from \"Hello World\" to real world. Along the way, there are a number of Resources available to support and accelerate your journey. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Getting started"},{"location":"getting-started/best-practices/","text":"Best Practices \u00b6 Todo Complete this page","title":"Best practices"},{"location":"getting-started/best-practices/#best-practices","text":"Todo Complete this page","title":"Best Practices"},{"location":"getting-started/overview/","text":"Getting Started - Overview \u00b6 The Toolkit provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Tutorial section. From there, the Tasks section provides more detailed guides to move from \"Hello World\" to real world. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Overview"},{"location":"getting-started/overview/#getting-started-overview","text":"The Toolkit provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Tutorial section. From there, the Tasks section provides more detailed guides to move from \"Hello World\" to real world. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Getting Started - Overview"},{"location":"getting-started/setup/","text":"Setup you learning and development environment \u00b6 The automation used within the toolkit uses a number of open source tools and utilities. To avoid issues with different operating systems, command line interpreters (shells) and versions of the various tools, it is recommended that a container or virtual machine is used, so you will have a verified working environment for the automation to run in. Some of the tools used are: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Todo what about other cloud CLIs? There are 2 recommended, supported environments: Docker Desktop Multipass Docker Desktop \u00b6 Docker Desktop provides a container environment for Windows and MacOS. It is free to use for non-commercial uses, but requires a license for commercial use. If the license isn't an issue then this is the simplest, recommended option to use. Multipass \u00b6 Multipass is a free virtual machine environment for Windows, MacOS and Linux to run Ubuntu images. Warning Some users have reported DNS resolution issues when using Multipass with some VPN clients, such as Cisco Anywhere. If you encounter such issues then you must work with the VPN client not running Additional options \u00b6 There are some additional environments that can be used, but these are not supported and cannot be guaranteed to work. There are also some known issues with the environments listed below: podman colima Podman \u00b6 Podman is an open source tool, free to use, that provides much the same functionality as Docker Desktop. There are some known issues with Podman: When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module. Colima \u00b6 Colima is an open source container engine for Intel or Arm based Mac systems. It is free to use, but there are some known issues with Colima: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Setup instructions \u00b6 Instructions for setting up your chosen container or VM environment are covered in the first tutorial","title":"Setup"},{"location":"getting-started/setup/#setup-you-learning-and-development-environment","text":"The automation used within the toolkit uses a number of open source tools and utilities. To avoid issues with different operating systems, command line interpreters (shells) and versions of the various tools, it is recommended that a container or virtual machine is used, so you will have a verified working environment for the automation to run in. Some of the tools used are: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Todo what about other cloud CLIs? There are 2 recommended, supported environments: Docker Desktop Multipass","title":"Setup you learning and development environment"},{"location":"getting-started/setup/#docker-desktop","text":"Docker Desktop provides a container environment for Windows and MacOS. It is free to use for non-commercial uses, but requires a license for commercial use. If the license isn't an issue then this is the simplest, recommended option to use.","title":"Docker Desktop"},{"location":"getting-started/setup/#multipass","text":"Multipass is a free virtual machine environment for Windows, MacOS and Linux to run Ubuntu images. Warning Some users have reported DNS resolution issues when using Multipass with some VPN clients, such as Cisco Anywhere. If you encounter such issues then you must work with the VPN client not running","title":"Multipass"},{"location":"getting-started/setup/#additional-options","text":"There are some additional environments that can be used, but these are not supported and cannot be guaranteed to work. There are also some known issues with the environments listed below: podman colima","title":"Additional options"},{"location":"getting-started/setup/#podman","text":"Podman is an open source tool, free to use, that provides much the same functionality as Docker Desktop. There are some known issues with Podman: When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module.","title":"Podman"},{"location":"getting-started/setup/#colima","text":"Colima is an open source container engine for Intel or Arm based Mac systems. It is free to use, but there are some known issues with Colima: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host","title":"Colima"},{"location":"getting-started/setup/#setup-instructions","text":"Instructions for setting up your chosen container or VM environment are covered in the first tutorial","title":"Setup instructions"},{"location":"getting-started/whats-new/","text":"What's new \u00b6","title":"What's new"},{"location":"getting-started/whats-new/#whats-new","text":"","title":"What's new"},{"location":"learn/","text":"Learn \u00b6 Learn how to use the TechZone Automation to provision and customize cloud resources, be it infrastructure or software, across a number of hosting environments. TechZone Automation Builder \u00b6 The TechZone Automation Builder is a user interface that provides a catalog of common reference architectures and wizards to combine and customize them for use in hybrid-cloud environments. Coming soon IasCable \u00b6 The logic for generating automation templates in TechZone Automation Builder is provided by IasCable. IasCable has been built to convert a listing of the required capabilities of an architecture (referred to as a Bill of Materials) and generate the necessary automation to bring the architecture to life. For most use cases, TechZone Automation Builder is the recommended way to work with the automation. However, IasCable provides a more advanced level of control over the definition of the architecture. Learn IasCable - Learn how to define your own Bill of Material and use IasCable to generate the automation.","title":"Overview"},{"location":"learn/#learn","text":"Learn how to use the TechZone Automation to provision and customize cloud resources, be it infrastructure or software, across a number of hosting environments.","title":"Learn"},{"location":"learn/#techzone-automation-builder","text":"The TechZone Automation Builder is a user interface that provides a catalog of common reference architectures and wizards to combine and customize them for use in hybrid-cloud environments. Coming soon","title":"TechZone Automation Builder"},{"location":"learn/#iascable","text":"The logic for generating automation templates in TechZone Automation Builder is provided by IasCable. IasCable has been built to convert a listing of the required capabilities of an architecture (referred to as a Bill of Materials) and generate the necessary automation to bring the architecture to life. For most use cases, TechZone Automation Builder is the recommended way to work with the automation. However, IasCable provides a more advanced level of control over the definition of the architecture. Learn IasCable - Learn how to define your own Bill of Material and use IasCable to generate the automation.","title":"IasCable"},{"location":"learn/iascable/","text":"Getting started with IasCable \u00b6 The getting started section provides you three initial getting started with IasCable and Technology Zone Accelerator Toolkit . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Technology Zone Accelerator Toolkit . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab 3: Use Technology Zone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud Lab 4: Develop an own GitOps module","title":"Overview"},{"location":"learn/iascable/#getting-started-with-iascable","text":"The getting started section provides you three initial getting started with IasCable and Technology Zone Accelerator Toolkit . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Technology Zone Accelerator Toolkit . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab 3: Use Technology Zone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud Lab 4: Develop an own GitOps module","title":"Getting started with IasCable"},{"location":"learn/iascable/lab1/","text":"Lab 1: Getting start with the basics \u00b6 This is just a starting point to use [IasCable](https://github.com/cloud-native-toolkit/iascable) . 1. The IasCable framework \u00b6 Let us have first a look the basic components of the IasCable framework. Bill of Material and Modules \u00b6 The IasCable uses a Bill of Material and Modules (from the Technology Zone Accelerator Toolkit project ), which you need to understand. These two parts are the heart of the framework we could say to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. Simplified we can say a BOM is specified by modules it uses, variables you can define and providers you can define. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Simplified we can say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework. Realize Technology Zone Accelerator Toolkit with Bill of Material and Modules \u00b6 Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the created Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies. 2. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 3. Step-by-step example setup \u00b6 This is a step by step example setup to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview Step 1: Install CLI \u00b6 curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Step 2: Verify the installation \u00b6 iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ] Step 3: Create a Bill of Materials (BOM) file \u00b6 nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets Step 4: Execute following command \u00b6 iascable build -i firstbom.yaml Step 5: Verify the created content \u00b6 \u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is table contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group Step 6: Execute the terraform init command \u00b6 Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init Step 7: Execute the terraform plan command \u00b6 Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Step 8: Execute the terraform apply \u00b6 Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed. Step 9: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed. 4. Summary \u00b6 The IasCable and the Modules (from the Technology Zone Accelerator Toolkit project) which are all from the Cloud Native Toolkit providing a good concept for a framework to provide reusable components to install and configure cloud infrastructure. This was just a getting started. There is more to learn.","title":"Lab 1: The basics"},{"location":"learn/iascable/lab1/#lab-1-getting-start-with-the-basics","text":"This is just a starting point to use [IasCable](https://github.com/cloud-native-toolkit/iascable) .","title":"Lab 1: Getting start with the basics"},{"location":"learn/iascable/lab1/#1-the-iascable-framework","text":"Let us have first a look the basic components of the IasCable framework.","title":"1. The IasCable framework"},{"location":"learn/iascable/lab1/#bill-of-material-and-modules","text":"The IasCable uses a Bill of Material and Modules (from the Technology Zone Accelerator Toolkit project ), which you need to understand. These two parts are the heart of the framework we could say to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. Simplified we can say a BOM is specified by modules it uses, variables you can define and providers you can define. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Simplified we can say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework.","title":"Bill of Material and Modules"},{"location":"learn/iascable/lab1/#realize-technology-zone-accelerator-toolkit-with-bill-of-material-and-modules","text":"Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the created Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies.","title":"Realize Technology Zone Accelerator Toolkit with Bill of Material and Modules"},{"location":"learn/iascable/lab1/#2-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"2. Pre-requisites for the example"},{"location":"learn/iascable/lab1/#3-step-by-step-example-setup","text":"This is a step by step example setup to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview","title":"3. Step-by-step example setup"},{"location":"learn/iascable/lab1/#step-1-install-cli","text":"curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Step 1: Install CLI"},{"location":"learn/iascable/lab1/#step-2-verify-the-installation","text":"iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ]","title":"Step 2: Verify the installation"},{"location":"learn/iascable/lab1/#step-3-create-a-bill-of-materialsbom-file","text":"nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets","title":"Step 3: Create a Bill of Materials(BOM) file"},{"location":"learn/iascable/lab1/#step-4-execute-following-command","text":"iascable build -i firstbom.yaml","title":"Step 4: Execute following command"},{"location":"learn/iascable/lab1/#step-5-verify-the-created-content","text":"\u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is table contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group","title":"Step 5: Verify the created content"},{"location":"learn/iascable/lab1/#step-6-execute-the-terraform-init-command","text":"Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init","title":"Step 6: Execute the terraform init command"},{"location":"learn/iascable/lab1/#step-7-execute-the-terraform-plan-command","text":"Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example .","title":"Step 7: Execute the terraform plan  command"},{"location":"learn/iascable/lab1/#step-8-execute-the-terraform-apply","text":"Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed.","title":"Step 8: Execute the terraform apply"},{"location":"learn/iascable/lab1/#step-9-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed.","title":"Step 9: Execute the terraform destroy command"},{"location":"learn/iascable/lab1/#4-summary","text":"The IasCable and the Modules (from the Technology Zone Accelerator Toolkit project) which are all from the Cloud Native Toolkit providing a good concept for a framework to provide reusable components to install and configure cloud infrastructure. This was just a getting started. There is more to learn.","title":"4. Summary"},{"location":"learn/iascable/lab2/","text":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud \u00b6 The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc 1. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 2. Step-by-step example setup \u00b6 This is a step by step example setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview Step 1: Write the Bill of Material BOM file \u00b6 nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 Step 2: Build the project based on Bill of Material BOM file \u00b6 iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output Step 3: Verify the created files and folders \u00b6 tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online . Step 4: Execute the terraform init command \u00b6 Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init Step 5: Execute the terraform apply command \u00b6 Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4 Step 6: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed. 3. Summary \u00b6 When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"Lab 2: Infrastructure"},{"location":"learn/iascable/lab2/#lab-2-use-iascable-to-create-a-vpc-and-a-red-hat-openshift-cluster-on-ibm-cloud","text":"The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc","title":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud"},{"location":"learn/iascable/lab2/#1-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"1. Pre-requisites for the example"},{"location":"learn/iascable/lab2/#2-step-by-step-example-setup","text":"This is a step by step example setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview","title":"2. Step-by-step example setup"},{"location":"learn/iascable/lab2/#step-1-write-the-bill-of-material-bom-file","text":"nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1","title":"Step 1: Write the Bill of Material BOM file"},{"location":"learn/iascable/lab2/#step-2-build-the-project-based-on-bill-of-material-bom-file","text":"iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output","title":"Step 2: Build the project based on Bill of Material BOM file"},{"location":"learn/iascable/lab2/#step-3-verify-the-created-files-and-folders","text":"tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online .","title":"Step 3: Verify the created files and folders"},{"location":"learn/iascable/lab2/#step-4-execute-the-terraform-init-command","text":"Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init","title":"Step 4: Execute the terraform init command"},{"location":"learn/iascable/lab2/#step-5-execute-the-terraform-apply-command","text":"Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4","title":"Step 5: Execute the terraform apply  command"},{"location":"learn/iascable/lab2/#step-6-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed.","title":"Step 6: Execute the terraform destroy command"},{"location":"learn/iascable/lab2/#3-summary","text":"When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"3. Summary"},{"location":"learn/iascable/lab3/","text":"Lab 3: Use Technology Zone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud \u00b6 Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The Technology Zone Accelerator Toolkit project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the Technology Zone Accelerator Toolkit catalog does provide. Surely, we need to know the needed outline for the cloud architecture which does depend on the cloud environment we are going to use. As I said the Technology Zone Accelerator Toolkit catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed Technology Zone Accelerator Toolkit Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helper scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find the in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example 1. Define an outline of the target architecture \u00b6 This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud 2. Identify the needed Technology Zone Accelerator Toolkit Terraform modules for the target architecture \u00b6 Let us first define which Technology Zone Accelerator Toolkit Terraform modules we are going to use for our custom BOM file specification. The Technology Zone Accelerator Toolkit project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure 1. Configuration of GitOps \u00b6 IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap 2. Cloud infrastructure/services resources related \u00b6 IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage 3. Write a customized BOM to combine the modules \u00b6 Step 1: Write the Bill of Material BOM file \u00b6 Now we combine the existing Terraform modules we got from the Technology Zone Accelerator Toolkit catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization. 4. Use IasCable to create the scaffolding for a Terraform project \u00b6 Step 1: Install colima container engine and start the container engine \u00b6 Example for an installation of colima on macOS. brew install docker colima colima start Step 2: Create a terraform project based on Bill of Material BOM file \u00b6 Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output Step 3: Copy helper bash scripts into the output folder \u00b6 cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output Step 4: Start the tools container provided by the IasCable \u00b6 Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh 5. Use the IasCable tools container to execute the Terraform modules \u00b6 Step 1 (inside the container): In the running container verify the mapped resources \u00b6 ~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it \u00b6 sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd 6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration \u00b6 Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume \u00b6 All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file . 7. Destroy the environment on IBM Cloud \u00b6 Step 1 (inside the container): Destroy the created IBM Cloud resources \u00b6 All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed. 8. Summary \u00b6 We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Technology Zone Accelerator Toolkit project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The Technology Zone Accelerator Toolkit project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Technology Zone Accelerator Toolkit with Terraform and contribute to the project.","title":"Lab 3: GitOps"},{"location":"learn/iascable/lab3/#lab-3-use-technology-zone-accelerator-toolkit-and-iascable-to-setup-gitops-on-a-red-hat-openshift-cluster-in-a-virtual-private-cloud-on-ibm-cloud","text":"Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The Technology Zone Accelerator Toolkit project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the Technology Zone Accelerator Toolkit catalog does provide. Surely, we need to know the needed outline for the cloud architecture which does depend on the cloud environment we are going to use. As I said the Technology Zone Accelerator Toolkit catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed Technology Zone Accelerator Toolkit Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helper scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find the in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example","title":"Lab 3: Use Technology Zone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud"},{"location":"learn/iascable/lab3/#1-define-an-outline-of-the-target-architecture","text":"This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud","title":"1. Define an outline of the target architecture"},{"location":"learn/iascable/lab3/#2-identify-the-needed-technology-zone-accelerator-toolkit-terraform-modules-for-the-target-architecture","text":"Let us first define which Technology Zone Accelerator Toolkit Terraform modules we are going to use for our custom BOM file specification. The Technology Zone Accelerator Toolkit project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure","title":"2. Identify the needed Technology Zone Accelerator Toolkit Terraform modules for the target architecture"},{"location":"learn/iascable/lab3/#1-configuration-of-gitops","text":"IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap","title":"1. Configuration of GitOps"},{"location":"learn/iascable/lab3/#2-cloud-infrastructureservices-resources-related","text":"IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage","title":"2. Cloud infrastructure/services resources related"},{"location":"learn/iascable/lab3/#3-write-a-customized-bom-to-combine-the-modules","text":"","title":"3. Write a customized BOM to combine the modules"},{"location":"learn/iascable/lab3/#step-1-write-the-bill-of-material-bom-file","text":"Now we combine the existing Terraform modules we got from the Technology Zone Accelerator Toolkit catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization.","title":"Step 1: Write the Bill of Material BOM file"},{"location":"learn/iascable/lab3/#4-use-iascable-to-create-the-scaffolding-for-a-terraform-project","text":"","title":"4. Use IasCable to create the scaffolding for a Terraform project"},{"location":"learn/iascable/lab3/#step-1-install-colima-container-engine-and-start-the-container-engine","text":"Example for an installation of colima on macOS. brew install docker colima colima start","title":"Step 1: Install colima container engine and start the container engine"},{"location":"learn/iascable/lab3/#step-2-create-a-terraform-project-based-on-bill-of-material-bom-file","text":"Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output","title":"Step 2: Create a terraform project based on Bill of Material BOM file"},{"location":"learn/iascable/lab3/#step-3-copy-helper-bash-scripts-into-the-output-folder","text":"cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output","title":"Step 3: Copy helper bash scripts into the output folder"},{"location":"learn/iascable/lab3/#step-4-start-the-tools-container-provided-by-the-iascable","text":"Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh","title":"Step 4: Start the tools container provided by the IasCable"},{"location":"learn/iascable/lab3/#5-use-the-iascable-tools-container-to-execute-the-terraform-modules","text":"","title":"5. Use the IasCable tools container to execute the Terraform modules"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-in-the-running-container-verify-the-mapped-resources","text":"~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd","title":"Step 1 (inside the container): In the running container verify the mapped resources"},{"location":"learn/iascable/lab3/#step-2-inside-the-container-create-a-workspace-folder-in-your-container-and-copy-your-iascable-project-into-it","text":"sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd","title":"Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it"},{"location":"learn/iascable/lab3/#6-apply-the-terraform-modules-to-create-the-environment-in-ibm-cloud-and-backup-terraform-configuration","text":"","title":"6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-execute-the-applysh-and-backup-the-result-into-the-mapped-volume","text":"All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file .","title":"Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume"},{"location":"learn/iascable/lab3/#7-destroy-the-environment-on-ibm-cloud","text":"","title":"7. Destroy the environment on IBM Cloud"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-destroy-the-created-ibm-cloud-resources","text":"All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed.","title":"Step 1 (inside the container): Destroy the created IBM Cloud resources"},{"location":"learn/iascable/lab3/#8-summary","text":"We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Technology Zone Accelerator Toolkit project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The Technology Zone Accelerator Toolkit project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Technology Zone Accelerator Toolkit with Terraform and contribute to the project.","title":"8. Summary"},{"location":"learn/iascable/lab4/","text":"Lab 4: Develop an own GitOps module \u00b6 1. Objective \u00b6 The objective is to understand how to build and use a custom module for the Technology Zone Accelerator Toolkit . Therefor a custom module will be created in GitOps scenario to deploy a helm-chart for an example application. The custom module will be deployed on a Red Hat OpenShift cluster on IBM Cloud with Argo CD configured for GitOps. 2. What does the repository do? \u00b6 This repository does inspect the template-terraform-gitops . The repository shows how to create a custom module for Technology Zone Accelerator Toolkit step-by-step using the ubi-helm example from the Argo CD GitHub repository. The repository shows how to use a custom module in a BOM (Bill of material) The repository shows how to create and use a custom catalog for a custom module The repository shows and inspects usage of a custom module 2.1 Understand the template-terraform-gitops \u00b6 The template-terraform-gitops is a part of the How to instructions of the Technology Zone Accelerator Toolkit . The module covers the GitOps topic . 3. Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example \u00b6 These are the main tasks: Create a GitHub repository based on the gitops template from Software Everywhere Configure the terraform-gitops-ubi module Create an own catalog for the terraform-gitops-ubi module Create a BOM (Bill of material) where the terraform-gitops-ubi module is used and create the needed terraform output with iascable We will use later two catalogs and one BOM (Bill of material). here is a simplified view of the dependencies we will have later. 3.1 Prepare the environment \u00b6 3.1.1 Create a new GitHub repository based on the gitops template \u00b6 We clone the gitops template repository to our local computer and we going to create our terraform-gitops-ubi repository. Step 1: Clone the GitHub gitops template repository to your local computer and create a new GitHub repository based on that template You can follow the steps in the blog post to do this. Then you should have following folder structure on on computer: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 module.yaml \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 create-yaml.sh \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 stages \u2502 \u251c\u2500\u2500 stage0.tf \u2502 \u251c\u2500\u2500 stage1-cert.tf \u2502 \u251c\u2500\u2500 stage1-cluster.tf \u2502 \u251c\u2500\u2500 stage1-cp-catalogs.tf \u2502 \u251c\u2500\u2500 stage1-gitops-bootstrap.tf \u2502 \u251c\u2500\u2500 stage1-gitops.tf \u2502 \u251c\u2500\u2500 stage1-namespace.tf \u2502 \u251c\u2500\u2500 stage2-mymodule.tf \u2502 \u251c\u2500\u2500 stage3-outputs.tf \u2502 \u2514\u2500\u2500 variables.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 3.1.2 Install iascable \u00b6 We install iascable to ensure you use the latest version. Step 1: Install iascable on your local computer curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh iascable --version Example output: 2 .17.4 3.1.2 Install a Multipass \u00b6 We will follow the instructions for Multipass . The following steps are an extractions of the cloud-native-toolkit documentation with small changes when needed. Step 1: Install Multipass with brew brew install --cask multipass Step 2: Download cloud-init configuration \u00b6 curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Step 3: Start the virtual cli-tools machine \u00b6 multipass launch --name cli-tools --cloud-init ./cli-tools.yaml 4. Implement the new terraform-gitops-ubi module \u00b6 In that section we will modify files in our newly created repository. These are the relevant files for our new module. The main.tf file The variable.tf file The helm chart content The module.yaml file Configure the helm chart copy automation in the scripts/create-yaml.sh file Create for terraform-gitops-ubi GitHub repository tags and releases 4.1 The main.tf file \u00b6 Step 1: Do some modifications in the main.tf file Change name = \"my-helm-chart-folder\" to ubi-helm First add ubi-helm = {// create entry} to the values_content = {} . That entry will be used to create the values for the variables in the values.yaml file for the helm chart. Below you see the relevant code in the main.tf which does the copy later. As you can is it uses the {local.name} value, so you need to ensure the name reflects the folder structure for your helm-chart later. resource null_resource create_yaml { provisioner \"local-exec\" { command = \" ${ path .module } /scripts/create-yaml.sh ' ${ local .name } ' ' ${ local .yaml_dir } '\" environment = { VALUES_CONTENT = yamlencode ( local.values_content ) } } } These are the values we need to insert for our terraform-gitops-ubi application as variables for the helm-chart. You find the variables in the Argo CD github project for the ubi-helm values.yaml Now replace the // create entry with the needed values. ubi-helm = { \"replicaCount\" : 1 \"image.repository\" = \"registry.access.redhat.com/ubi8/ubi\" } Change layer = \"services\" to layer = \"applications\" Add cluster_type = var.cluster_type == \"kubernetes\" ? \"kubernetes\" : \"openshift\" to the locals Resulting locals section in the main.tf file locals { na me = \"ubi-helm\" bi n _dir = module.se tu p_clis.bi n _dir yaml_dir = \"${path.cwd}/.tmp/${local.name}/chart/${local.name}\" service_url = \"http://${local.name}.${var.namespace}\" clus ter _ t ype = var.clus ter _ t ype == \"kubernetes\" ? \"kubernetes\" : \"openshift\" values_co ntent = { ubi - helm= { \"replicaCount\" : 1 \"image.repository\" = \"registry.access.redhat.com/ubi8/ubi\" \"image.tag\" = \"latest\" } } layer = \"applications\" t ype = \"base\" applica t io n _bra n ch = \"main\" na mespace = var. na mespace layer_co nf ig = var.gi t ops_co nf ig [ local.layer ] } 4.2 The variable.tf file \u00b6 Step 1: Add some variables in the variable.tf file variable \"cluster_type\" { description = \"The cluster type (openshift or kubernetes)\" default = \"openshift\" } 4.3 The helm chart content \u00b6 Step 1: Create a new folder structure for the terraform-gitops-ubi helm chart Create following folder structure chart/ubi-helm . The name after chart must be the module name. \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 charts \u2502 \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u2502 \u2514\u2500\u2500 deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 ubi-helm-v0.0.01.tgz \u2502 \u2502 \u2514\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 values.yaml That will be the resulting folder structure for the terraform-gitops-ubi module on your local pc: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 charts \u2502 \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u2502 \u2514\u2500\u2500 deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 ubi-helm-v0.0.1.tgz \u2502 \u2502 \u2514\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 module.yaml \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 create-yaml.sh \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 stages \u2502 \u251c\u2500\u2500 stage0.tf \u2502 \u251c\u2500\u2500 stage1-cert.tf \u2502 \u251c\u2500\u2500 stage1-cluster.tf \u2502 \u251c\u2500\u2500 stage1-cp-catalogs.tf \u2502 \u251c\u2500\u2500 stage1-gitops-bootstrap.tf \u2502 \u251c\u2500\u2500 stage1-gitops.tf \u2502 \u251c\u2500\u2500 stage1-namespace.tf \u2502 \u251c\u2500\u2500 stage2-mymodule.tf \u2502 \u251c\u2500\u2500 stage3-outputs.tf \u2502 \u2514\u2500\u2500 variables.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf Step 2: Copy in newly create folder structure the content from the repository for the ubi-helm chart https://github.com/thomassuedbroecker/ubi-helm/tree/main/charts/ubi-helm Step 3: Validate the helm chart with following commands: Navigate the charts directory CHARTDIR = ./chart/ubi-helm/charts/ubi-helm cd $CHARTDIR Verify the dependencies helm dep update . Verify the helm chart structure helm lint . Example output: == > Linting . [ INFO ] Chart.yaml: icon is recommended 1 chart ( s ) linted, 0 chart ( s ) failed helm template test . -n test Example output: # Source: terraform-gitops-ubi/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: test-terraform-gitops-ubi labels: app: terraform-gitops-ubi chart: test-terraform-gitops-ubi release: test heritage: Helm spec: replicas: 2 revisionHistoryLimit: 3 selector: matchLabels: app: terraform-gitops-ubi release: test template: metadata: labels: app: terraform-gitops-ubi release: test spec: containers: - name: terraform-gitops-ubi image: \"registry.access.redhat.com/ubi8/ubi:latest\" imagePullPolicy: Always args: - /bin/sh - -c - touch /tmp/healthy ; sleep 30 ; rm -f /tmp/healthy ; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 helm package . 4.4 The module.yaml file \u00b6 Step 1: Edited the module.yaml Use for name : terraform-gitops-ubi Use for description : That module will add a new Argo CD config to deploy the terraform-gitops-ubi application name : \"terraform-gitops-ubi\" type : gitops description : \"That module will add a new Argo CD config to deploy the terraform-gitops-ubi application\" tags : - tools - gitops versions : - platforms : - kubernetes - ocp3 - ocp4 dependencies : - id : gitops refs : - source : github.com/cloud-native-toolkit/terraform-tools-gitops.git version : \">= 1.1.0\" - id : namespace refs : - source : github.com/cloud-native-toolkit/terraform-gitops-namespace.git version : \">= 1.0.0\" variables : - name : gitops_config moduleRef : id : gitops output : gitops_config - name : git_credentials moduleRef : id : gitops output : git_credentials - name : server_name moduleRef : id : gitops output : server_name - name : namespace moduleRef : id : namespace output : name - name : kubeseal_cert moduleRef : id : gitops output : sealed_secrets_cert 4.5 Configure the helm chart copy automation in the scripts/create-yaml.sh file \u00b6 Step 1: Configure the scripts/create-yaml.sh in terraform-gitops-ubi repository Replace the existing code in scripts/create-yaml.sh with following content. This is important for later when the helm-chart will be copied. #!/usr/bin/env bash SCRIPT_DIR = $( cd $( dirname \" $0 \" ) ; pwd -P ) MODULE_DIR = $( cd \" ${ SCRIPT_DIR } /..\" ; pwd -P ) CHART_DIR = $( cd \" ${ SCRIPT_DIR } /../chart/ubi-helm\" ; pwd -P ) NAME = \" $1 \" DEST_DIR = \" $2 \" ## Add logic here to put the yaml resource content in DEST_DIR mkdir -p \" ${ DEST_DIR } \" cp -R \" ${ CHART_DIR } /\" * \" ${ DEST_DIR } \" if [[ -n \" ${ VALUES_CONTENT } \" ]] ; then echo \" ${ VALUES_CONTENT } \" > \" ${ DEST_DIR } /values.yaml\" fi find \" ${ DEST_DIR } \" -name \"*\" echo \"Files in output path\" ls -l \" ${ DEST_DIR } \" 4.6 terraform-gitops-ubi GitHub repository tags and releases \u00b6 The release tag represents the version number of our module. terraform-gitops-ubi Step 1: Create GitHub tag and release for the terraform-gitops-ubi GitHub repository The module github repository release tags should be updated when you are going to change the terraform-gitops-ubi GitHub repository module. The image below shows some releases and as you can see for each release an archive is available. Later iascable uses the release tag to download the right archive to the local computer to create the Terraform output. In case when you use specific version numbers in the BOM which uses the module, you need to ensure that version number is also in range of the custom chart which points to the module. That is also relevant for the catalog.yaml we will define later. Example relevant extract from a BOM -> version: v0.0.5 # Install terraform-gitops-ubi # New custom module linked be the custom catalog - name : terraform-gitops-ubi alias : terraform-gitops-ubi version : v0.0.5 You can follow the step to create a GitHub tag is that example blog post and then create a release. 5. Create an own catalog \u00b6 In that example we will not publish the our terraform-gitops-ubi module to the public catalog on Technology Zone Accelerator Toolkit . We will create our own catalog.yaml file and save the configuration in the GitHub project of the module. How to create catalog.yaml file? How to combine various catalogs? Inspect the structure of a catalog.yaml Create a custom catalog steps The following diagram shows the simplified dependencies of module , catalog and iascable : 5.1 How to create catalog.yaml file? \u00b6 It is useful to take a look into iascable documentation and the build-catalog.sh automation . 5.2 How to combine various catalogs? \u00b6 You can combine more than one catalog resources and BOM inputs with the iascable build command. Here is the build command: iascable build [ -c { CATALOG_URL }] [ -c { CATALOG_URL }] -i { BOM_INPUT } [ -i { BOM_INPUT }] [ -o { OUTPUT_DIR }] CATALOG_URL is the url of the module catalog. The default module catalog is https://modules.cloudnativetoolkit.dev/index.yaml. Multiple module catalogs can be provided. The catalogs are combined, with the last one taking precedence in the case of duplicate modules. BOM_INPUT is the input file containing the Bill of Material definition. Multiple BOM files can be provided at the same time. OUTPUT_DIR is the directory where the output terraform template will be generated. 5.3 Inspect the structure of a catalog.yaml \u00b6 The structure of a catalog can be verified here https://modules.cloudnativetoolkit.dev/index.yaml That is a minimize extraction of the index.yaml above. It contains: categories , modules , aliases and providers . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : ai-ml - category : cluster - category : databases - category : dev-tool - category : gitops categoryName : GitOps selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops name : gitops-ocs-operator description : Module to populate a gitops repo with the resources to provision ocs-operator tags : - tools - gitops versions : [] id : github.com/cloud-native-toolkit/terraform-gitops-ocs-operator group : \"\" displayName : ocs-operator - category : iam - category : image-registry - category : infrastructure ... aliases : - id : github.com/terraform-ibm-modules/terraform-ibm-toolkit-mongodb ... providers : - name : ibm source : ibm-cloud/ibm variables : - name : ibmcloud_api_key scope : global - name : region scope : global 5.4 Inspect the module section of the catalog file in more detail \u00b6 We see that the modules section does contain following cloudProvider , softwareProvider , id , group , displayName and type which are not a part of the module.yaml . After these entries we insert content of the module.yaml . Current gitops template . 5.5 Create a custom catalog \u00b6 Step 1: Create a terraform-gitops-ubi-catalog.yml and insert following content Note: Ensure that the github project has a tag and a release! The right value of the release must be reference in the catalog! (Example version: v0.0.1 ). apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : examples categoryName : examples selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops id : github.com/Vishal-Ramani/terraform-gitops-ubi group : \"\" displayName : terraform-gitops-ubi name : terraform-gitops-ubi description : \"That module will add a new 'Argo CD config' to deploy a 'ubi' container to OpenShift\" tags : - tools - gitops versions : - platforms : - kubernetes - ocp3 - ocp4 version : v0.0.2 dependencies : - id : gitops refs : - source : github.com/cloud-native-toolkit/terraform-tools-gitops.git version : '>= 1.1.0' - id : namespace refs : - source : github.com/cloud-native-toolkit/terraform-gitops-namespace.git version : '>= 1.0.0' variables : - name : gitops_config type : |- object({ boostrap = object({ argocd-config = object({ project = string repo = string url = string path = string }) }) infrastructure = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) services = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) applications = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) }) description : Config information regarding the gitops repo structure moduleRef : id : gitops output : gitops_config - name : git_credentials type : |- list(object({ repo = string url = string username = string token = string })) description : The credentials for the gitops repo(s) sensitive : true moduleRef : id : gitops output : git_credentials - name : namespace type : string description : The namespace where the application should be deployed moduleRef : id : namespace output : name - name : kubeseal_cert type : string description : The certificate/public key used to encrypt the sealed secrets default : \"\" moduleRef : id : gitops output : sealed_secrets_cert - name : server_name type : string description : The name of the server default : default moduleRef : id : gitops output : server_name - name : cluster_type description : The cluster type (openshift or kubernetes) default : '\"openshift\"' outputs : - name : name description : The name of the module - name : branch description : The branch where the module config has been placed - name : namespace description : The namespace where the module will be deployed - name : server_name description : The server where the module will be deployed - name : layer description : The layer where the module is deployed - name : type description : The type of module where the module is deployed 5.6. BOM that we will use terraform-gitops-ubi module \u00b6 Step 1: Clone the project with the example BOM configuration git clone https://github.com/Vishal-Ramani/UBI-helm-module-example.git cd example Step 2: Verify the ibm-vpc-roks-argocd-ubi.yaml BOM file This is the simplified target architecture what our BOM will create as terraform code for initial setup. A customized IBM Cloud environment for GitOps and our terraform-gitops-ubi module. For the configuration of GitOps in Red Hat OpenShift. We will use two operators: Red Hat OpenShift GitOps operator We will create one ArgoCD instance with the Red at OpenShift GitOps operator, that ArgoCD instance will bin initial configured by a newly created GitHub project configure by a Cloud Native Toolkit template for GitOps repositories. Red Hat OpenShift Pipelines operator There will be no initial setup for a Tekton pipeline at the moment. That images show a simplified view of the Argo CD basic configuration. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud This is the structure of the BOM we are going to use: Virtual Private Cloud - related ROKS - related (RedHat OpenShift on IBM Cloud) GitOps and Bootstrap of GitOps Our own module called terraform-gitops-ubi Note: You need configure variables to your needs, when you share your IBM Cloud environment with others. We commented out the # version: v0.0.5 of our module, because we will configure only one version in our catalog.yaml which we will define later. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc-roks-argocd-ubi spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.1 variables : - name : name value : \"tsued-gitops-ubi\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-ubi\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.7 variables : - name : name value : \"tsued-gitops-ubi\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_ubi\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued_ubi\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops-ubi\" - name : repo value : \"iascable-gitops-ubi\" # Install ubi # New custom module linked be the custom catalog - name : terraform-gitops-ubi alias : terraform-gitops-ubi # version: v0.0.5 6. Create terraform code and create the resources \u00b6 Use iascable to create the terraform code. Step 1: Create a credentials.properties file and edit the file \u00b6 cd example cp ./credentials.properties-template ./credentials.properties nano credentials.properties Provide the your GitHub access token and IBM Cloud API key. export TF_VAR_gitops_repo_token = XXX export TF_VAR_ibmcloud_api_key = XXX Step 2: Execute following commands \u00b6 BASE_CATALOG = https://modules.cloudnativetoolkit.dev/index.yaml CUSTOM_CATALOG = https://raw.githubusercontent.com/Vishal-Ramani/UBI-helm-module-example/main/example/catalog/ubi-helm-catalog.yaml iascable build -i ibm-vpc-roks-argocd-ubi.yaml -c $BASE_CATALOG -c $CUSTOM_CATALOG Step 3: Verify the created files and folders \u00b6 tree . Example output: . \u251c\u2500\u2500 catalog \u2502 \u2514\u2500\u2500 ubi-helm-catalog.yaml \u251c\u2500\u2500 credentials.properties \u251c\u2500\u2500 credentials.properties-template \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 argocd-bootstrap.md \u2502 \u2502 \u251c\u2500\u2500 gitops-namespace.md \u2502 \u2502 \u251c\u2500\u2500 gitops-repo.md \u2502 \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc.md \u2502 \u2502 \u251c\u2500\u2500 olm.md \u2502 \u2502 \u251c\u2500\u2500 sealed-secret-cert.md \u2502 \u2502 \u2514\u2500\u2500 terraform-gitops-ubi.md \u2502 \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh 5 directories, 26 files Step 4: Navigate to the output folder \u00b6 cd output Step 5: Copy the credentials.properties into the output folder \u00b6 CURRENT_PATH = $( pwd ) PROJECT = ibm-vpc-roks-argocd-ubi cp $CURRENT_PATH /../credentials.properties $CURRENT_PATH /ibm-vpc-roks-argocd-ubi/credentials.properties Step 6: Map the current folder to the Multipass cli-tools VM \u00b6 Ensure you started the Multipass cli-tools VM before you execute the following command: multipass mount $PWD cli-tools:/automation Now we have mapped the output folder to the cli-tools VM . We can use the installed CLI tools inside the cli-tools VM to apply the Terraform code. All changes we made in with cli-tools VM will be saved in the mapped output folder on our local machine. Step 7: Open the interactive shell \u00b6 multipass shell cli-tools Example output: Last login: Mon Sep 12 18 :06:24 2022 from 192 .168.64.1 ubuntu@cli-tools:~$ Step 8: In the virtual machine navigate to the automation folder \u00b6 cd ../../automation ls Step 9: Now navigate to the ibm-vpc-roks-argocd-ubi folder \u00b6 cd ibm-vpc-roks-argocd-ubi/ ls Step 10: Source the credentials.properties as environment variables and show one variable \u00b6 source credentials.properties echo $TF_VAR_ibmcloud_api_key Step 11: Execute ./apply.sh \u00b6 ./apply.sh Step 12: Enter yes to apply the Terraform code \u00b6 Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Step 13: Interactive terminal actions \u00b6 These values you need to edit: Namespace: ubi-helm Region: eu-de Resource group: default gitops-repo_token: XXX Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops-ubi ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXXX > Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'common_tags' : Common tags that should be added to the instance > ([]) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Provide a value for 'namespace_name' : The value that should be used for the namespace > ubi-helm Step 14: Verify the output of terraform execution After some time you should get following output: Apply complete! Resources: 103 added, 0 changed, 0 destroyed. Step 15: Open Argo CD in OpenShift and verify the application instances Follow the steps in the shown in the gif . Step 16: Access the UBI pod in OpenShift and execute ls in the terminal Follow the steps in the shown in the gif . 7. Verify the created Argo CD configuration on GitHub \u00b6 We see that in our GitHub account new repository was created from the GitOps bootstrap module and the terraform-tools-gitops module to figure Argo CD for by using the app-of-apps concept with a single GitHub repository to manage all Argo CD application configuration and helm configurations to deploy applications in the GitOps context. Reminder the boot strap configuration is shown in the following image for details visit the terraform-tools-gitops module. The new GitHub repository is called iascable-gitops-ubi in our case. The new iascable-gitops-ubi repository contains two folders the following image shows the relation to the bootstrap configuration. argocd folder which contains the configuration for Argo CD let us call it app-of-apps folder. The following image displays the resulting configuration in Argo CD payload folder which contains the current helm deployment for the apps which will be deployed. The following image show the deployment created by apps in our case the ubi-helm. The following image shows the newly created GitHub iascable-gitops-ubi repository. For more details visit the template of the terraform-tools-gitops module. 7.1 Understand how the ubi module content was pasted into the new iascable-gitops-ubi repository \u00b6 Following the concept for the gitops bootstrap setup documented in the template-terraform-gitops GitHub repository. We have two main folders in the iascable-gitops-ubi repository. One for the Argo CD application configurations called argocd One for the application which will be deployed be the Argo CD application configurations called payload. Let us inspect these two folders. The gif below shows some of the created files and folders. 7.1.1 argocd folder There were two Argo CD application configurations added into the iascable-gitops-ubi repository. One for the namespace in the OpenShift or Kubernetes cluster where the ubi application will be deployed. That Argo CD application configuration is related to exiting 1-infrastructure Argo CD project created by the GitOps bootstrap module . One for the ubi application we want to deploy. That Argo CD application configuration is related to exiting 3-application Argo CD project created by the GitOps bootstrap module . Let's take a look a the created Argo CD application configurations We have two Argo CD application configurations: 7.1.1.1 ubi Namespace in argocd.1-infrastructure.cluster.default.base.namespace-ubi-helm.yaml apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ubi-helm finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : default server : https://kubernetes.default.svc project : 1-infrastructure source : path : payload/1-infrastructure/namespace/ubi-helm/namespace repoURL : https://github.com/thomassuedbroecker/iascable-gitops-ubi.git targetRevision : main syncPolicy : automated : prune : true selfHeal : true ignoreDifferences : [] 7.1.1.2 UBI application deployment argocd.3-applications.cluster.default.base.ubi-helm-ubi.yaml This is the Argo CD application configuration ubi-helm-ubi-helm.yaml file, which was created automatically by our module with the igc gitops-module command. That payload directory is used as the source.path in that Argo CD application configuration as you see above. apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ubi-helm-ubi-helm finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ubi-helm server : https://kubernetes.default.svc project : 3-applications source : path : payload/3-applications/namespace/ubi-helm/ubi-helm repoURL : https://github.com/thomassuedbroecker/iascable-gitops-ubi.git targetRevision : main helm : releaseName : ubi-helm syncPolicy : automated : prune : true selfHeal : true ignoreDifferences : [] 7.1.2 payload folder That folder contains a namespace payload and the helm-chart payload. 7.1.2.2 ubi Namespace in payload.1-infrastructure.cluster.default.base In the folder payload.1-infrastructure.cluster.default.base we have an ns.yaml and rbac.yaml . ns.yaml apiVersion : v1 kind : Namespace metadata : name : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-30\" --- apiVersion : operators.coreos.com/v1 kind : OperatorGroup metadata : name : ubi-helm-operator-group namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" spec : targetNamespaces : - ubi-helm rbac.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : argocd-admin namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" rules : - apiGroups : - \"*\" resources : - \"*\" verbs : - \"*\" --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : argocd-admin namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : argocd-admin subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:openshift-gitops 7.1.1.2 ubi helm application deployment payload.3-applications.cluster.default.base That folder contains the ubi application helm chart configuration to deploy the ubi application. The script scripts/create-yaml.sh of our module terraform-gitops-ubi was responsible to copy the ubi helm-chart into the payload directory. Therefor we did the customization of that file. We defined the values content for the helm chart variables before in the module.tf file. That file values.yaml file is used in Argo CD application configuration for the parameters. values_content = { ubi-helm = { // create entry } } The following gif shows the relation of the parameter configuration for the helm-chart","title":"Lab 4: Develop GitOps module"},{"location":"learn/iascable/lab4/#lab-4-develop-an-own-gitops-module","text":"","title":"Lab 4: Develop an own GitOps module"},{"location":"learn/iascable/lab4/#1-objective","text":"The objective is to understand how to build and use a custom module for the Technology Zone Accelerator Toolkit . Therefor a custom module will be created in GitOps scenario to deploy a helm-chart for an example application. The custom module will be deployed on a Red Hat OpenShift cluster on IBM Cloud with Argo CD configured for GitOps.","title":"1. Objective"},{"location":"learn/iascable/lab4/#2-what-does-the-repository-do","text":"This repository does inspect the template-terraform-gitops . The repository shows how to create a custom module for Technology Zone Accelerator Toolkit step-by-step using the ubi-helm example from the Argo CD GitHub repository. The repository shows how to use a custom module in a BOM (Bill of material) The repository shows how to create and use a custom catalog for a custom module The repository shows and inspects usage of a custom module","title":"2. What does the repository do?"},{"location":"learn/iascable/lab4/#21-understand-the-template-terraform-gitops","text":"The template-terraform-gitops is a part of the How to instructions of the Technology Zone Accelerator Toolkit . The module covers the GitOps topic .","title":"2.1 Understand the template-terraform-gitops"},{"location":"learn/iascable/lab4/#3-use-the-template-terraform-gitops-to-create-a-module-to-deploy-the-terraform-gitops-ubi-example","text":"These are the main tasks: Create a GitHub repository based on the gitops template from Software Everywhere Configure the terraform-gitops-ubi module Create an own catalog for the terraform-gitops-ubi module Create a BOM (Bill of material) where the terraform-gitops-ubi module is used and create the needed terraform output with iascable We will use later two catalogs and one BOM (Bill of material). here is a simplified view of the dependencies we will have later.","title":"3. Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example"},{"location":"learn/iascable/lab4/#31-prepare-the-environment","text":"","title":"3.1 Prepare the environment"},{"location":"learn/iascable/lab4/#311-create-a-new-github-repository-based-on-the-gitops-template","text":"We clone the gitops template repository to our local computer and we going to create our terraform-gitops-ubi repository.","title":"3.1.1 Create a new GitHub repository based on the gitops template"},{"location":"learn/iascable/lab4/#312-install-iascable","text":"We install iascable to ensure you use the latest version.","title":"3.1.2 Install iascable"},{"location":"learn/iascable/lab4/#312-install-a-multipass","text":"We will follow the instructions for Multipass . The following steps are an extractions of the cloud-native-toolkit documentation with small changes when needed.","title":"3.1.2 Install a Multipass"},{"location":"learn/iascable/lab4/#step-2-download-cloud-init-configuration","text":"curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml","title":"Step 2: Download cloud-init configuration"},{"location":"learn/iascable/lab4/#step-3-start-the-virtual-cli-tools-machine","text":"multipass launch --name cli-tools --cloud-init ./cli-tools.yaml","title":"Step 3: Start the virtual cli-tools machine"},{"location":"learn/iascable/lab4/#4-implement-the-new-terraform-gitops-ubi-module","text":"In that section we will modify files in our newly created repository. These are the relevant files for our new module. The main.tf file The variable.tf file The helm chart content The module.yaml file Configure the helm chart copy automation in the scripts/create-yaml.sh file Create for terraform-gitops-ubi GitHub repository tags and releases","title":"4. Implement the new terraform-gitops-ubi module"},{"location":"learn/iascable/lab4/#41-the-maintf-file","text":"","title":"4.1 The main.tf file"},{"location":"learn/iascable/lab4/#42-the-variabletf-file","text":"","title":"4.2 The variable.tf file"},{"location":"learn/iascable/lab4/#43-the-helm-chart-content","text":"","title":"4.3 The helm chart content"},{"location":"learn/iascable/lab4/#44-the-moduleyaml-file","text":"","title":"4.4 The module.yaml file"},{"location":"learn/iascable/lab4/#45-configure-the-helm-chart-copy-automation-in-the-scriptscreate-yamlsh-file","text":"","title":"4.5 Configure the helm chart copy automation in the scripts/create-yaml.sh file"},{"location":"learn/iascable/lab4/#46-terraform-gitops-ubi-github-repository-tags-and-releases","text":"The release tag represents the version number of our module. terraform-gitops-ubi","title":"4.6 terraform-gitops-ubi GitHub repository tags and releases"},{"location":"learn/iascable/lab4/#5-create-an-own-catalog","text":"In that example we will not publish the our terraform-gitops-ubi module to the public catalog on Technology Zone Accelerator Toolkit . We will create our own catalog.yaml file and save the configuration in the GitHub project of the module. How to create catalog.yaml file? How to combine various catalogs? Inspect the structure of a catalog.yaml Create a custom catalog steps The following diagram shows the simplified dependencies of module , catalog and iascable :","title":"5. Create an own catalog"},{"location":"learn/iascable/lab4/#51-how-to-create-catalogyaml-file","text":"It is useful to take a look into iascable documentation and the build-catalog.sh automation .","title":"5.1 How to create catalog.yaml file?"},{"location":"learn/iascable/lab4/#52-how-to-combine-various-catalogs","text":"You can combine more than one catalog resources and BOM inputs with the iascable build command. Here is the build command: iascable build [ -c { CATALOG_URL }] [ -c { CATALOG_URL }] -i { BOM_INPUT } [ -i { BOM_INPUT }] [ -o { OUTPUT_DIR }] CATALOG_URL is the url of the module catalog. The default module catalog is https://modules.cloudnativetoolkit.dev/index.yaml. Multiple module catalogs can be provided. The catalogs are combined, with the last one taking precedence in the case of duplicate modules. BOM_INPUT is the input file containing the Bill of Material definition. Multiple BOM files can be provided at the same time. OUTPUT_DIR is the directory where the output terraform template will be generated.","title":"5.2 How to combine various catalogs?"},{"location":"learn/iascable/lab4/#53-inspect-the-structure-of-a-catalogyaml","text":"The structure of a catalog can be verified here https://modules.cloudnativetoolkit.dev/index.yaml That is a minimize extraction of the index.yaml above. It contains: categories , modules , aliases and providers . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : ai-ml - category : cluster - category : databases - category : dev-tool - category : gitops categoryName : GitOps selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops name : gitops-ocs-operator description : Module to populate a gitops repo with the resources to provision ocs-operator tags : - tools - gitops versions : [] id : github.com/cloud-native-toolkit/terraform-gitops-ocs-operator group : \"\" displayName : ocs-operator - category : iam - category : image-registry - category : infrastructure ... aliases : - id : github.com/terraform-ibm-modules/terraform-ibm-toolkit-mongodb ... providers : - name : ibm source : ibm-cloud/ibm variables : - name : ibmcloud_api_key scope : global - name : region scope : global","title":"5.3 Inspect the structure of a catalog.yaml"},{"location":"learn/iascable/lab4/#54-inspect-the-module-section-of-the-catalog-file-in-more-detail","text":"We see that the modules section does contain following cloudProvider , softwareProvider , id , group , displayName and type which are not a part of the module.yaml . After these entries we insert content of the module.yaml . Current gitops template .","title":"5.4 Inspect the module section of the catalog file in more detail"},{"location":"learn/iascable/lab4/#55-create-a-custom-catalog","text":"","title":"5.5 Create a custom catalog"},{"location":"learn/iascable/lab4/#56-bom-that-we-will-use-terraform-gitops-ubi-module","text":"","title":"5.6. BOM that we will use terraform-gitops-ubi module"},{"location":"learn/iascable/lab4/#6-create-terraform-code-and-create-the-resources","text":"Use iascable to create the terraform code.","title":"6. Create terraform code and create the resources"},{"location":"learn/iascable/lab4/#step-1-create-a-credentialsproperties-file-and-edit-the-file","text":"cd example cp ./credentials.properties-template ./credentials.properties nano credentials.properties Provide the your GitHub access token and IBM Cloud API key. export TF_VAR_gitops_repo_token = XXX export TF_VAR_ibmcloud_api_key = XXX","title":"Step 1: Create a credentials.properties file and edit the file"},{"location":"learn/iascable/lab4/#step-2-execute-following-commands","text":"BASE_CATALOG = https://modules.cloudnativetoolkit.dev/index.yaml CUSTOM_CATALOG = https://raw.githubusercontent.com/Vishal-Ramani/UBI-helm-module-example/main/example/catalog/ubi-helm-catalog.yaml iascable build -i ibm-vpc-roks-argocd-ubi.yaml -c $BASE_CATALOG -c $CUSTOM_CATALOG","title":"Step 2: Execute following commands"},{"location":"learn/iascable/lab4/#step-3-verify-the-created-files-and-folders","text":"tree . Example output: . \u251c\u2500\u2500 catalog \u2502 \u2514\u2500\u2500 ubi-helm-catalog.yaml \u251c\u2500\u2500 credentials.properties \u251c\u2500\u2500 credentials.properties-template \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 argocd-bootstrap.md \u2502 \u2502 \u251c\u2500\u2500 gitops-namespace.md \u2502 \u2502 \u251c\u2500\u2500 gitops-repo.md \u2502 \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc.md \u2502 \u2502 \u251c\u2500\u2500 olm.md \u2502 \u2502 \u251c\u2500\u2500 sealed-secret-cert.md \u2502 \u2502 \u2514\u2500\u2500 terraform-gitops-ubi.md \u2502 \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh 5 directories, 26 files","title":"Step 3: Verify the created files and folders"},{"location":"learn/iascable/lab4/#step-4-navigate-to-the-output-folder","text":"cd output","title":"Step 4: Navigate to the output folder"},{"location":"learn/iascable/lab4/#step-5-copy-the-credentialsproperties-into-the-output-folder","text":"CURRENT_PATH = $( pwd ) PROJECT = ibm-vpc-roks-argocd-ubi cp $CURRENT_PATH /../credentials.properties $CURRENT_PATH /ibm-vpc-roks-argocd-ubi/credentials.properties","title":"Step 5: Copy the credentials.properties into the output folder"},{"location":"learn/iascable/lab4/#step-6-map-the-current-folder-to-the-multipass-cli-tools-vm","text":"Ensure you started the Multipass cli-tools VM before you execute the following command: multipass mount $PWD cli-tools:/automation Now we have mapped the output folder to the cli-tools VM . We can use the installed CLI tools inside the cli-tools VM to apply the Terraform code. All changes we made in with cli-tools VM will be saved in the mapped output folder on our local machine.","title":"Step 6: Map the current folder to the Multipass cli-tools VM"},{"location":"learn/iascable/lab4/#step-7-open-the-interactive-shell","text":"multipass shell cli-tools Example output: Last login: Mon Sep 12 18 :06:24 2022 from 192 .168.64.1 ubuntu@cli-tools:~$","title":"Step 7: Open the interactive shell"},{"location":"learn/iascable/lab4/#step-8-in-the-virtual-machine-navigate-to-the-automation-folder","text":"cd ../../automation ls","title":"Step 8: In the virtual machine navigate to the automation folder"},{"location":"learn/iascable/lab4/#step-9-now-navigate-to-the-ibm-vpc-roks-argocd-ubi-folder","text":"cd ibm-vpc-roks-argocd-ubi/ ls","title":"Step 9: Now navigate to the ibm-vpc-roks-argocd-ubi folder"},{"location":"learn/iascable/lab4/#step-10-source-the-credentialsproperties-as-environment-variables-and-show-one-variable","text":"source credentials.properties echo $TF_VAR_ibmcloud_api_key","title":"Step 10: Source the credentials.properties as environment variables and show one variable"},{"location":"learn/iascable/lab4/#step-11-execute-applysh","text":"./apply.sh","title":"Step 11: Execute ./apply.sh"},{"location":"learn/iascable/lab4/#step-12-enter-yes-to-apply-the-terraform-code","text":"Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value:","title":"Step 12: Enter yes to apply the Terraform code"},{"location":"learn/iascable/lab4/#step-13-interactive-terminal-actions","text":"These values you need to edit: Namespace: ubi-helm Region: eu-de Resource group: default gitops-repo_token: XXX Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops-ubi ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXXX > Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'common_tags' : Common tags that should be added to the instance > ([]) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Provide a value for 'namespace_name' : The value that should be used for the namespace > ubi-helm","title":"Step 13: Interactive terminal actions"},{"location":"learn/iascable/lab4/#7-verify-the-created-argo-cd-configuration-on-github","text":"We see that in our GitHub account new repository was created from the GitOps bootstrap module and the terraform-tools-gitops module to figure Argo CD for by using the app-of-apps concept with a single GitHub repository to manage all Argo CD application configuration and helm configurations to deploy applications in the GitOps context. Reminder the boot strap configuration is shown in the following image for details visit the terraform-tools-gitops module. The new GitHub repository is called iascable-gitops-ubi in our case. The new iascable-gitops-ubi repository contains two folders the following image shows the relation to the bootstrap configuration. argocd folder which contains the configuration for Argo CD let us call it app-of-apps folder. The following image displays the resulting configuration in Argo CD payload folder which contains the current helm deployment for the apps which will be deployed. The following image show the deployment created by apps in our case the ubi-helm. The following image shows the newly created GitHub iascable-gitops-ubi repository. For more details visit the template of the terraform-tools-gitops module.","title":"7. Verify the created Argo CD configuration on GitHub"},{"location":"learn/iascable/lab4/#71-understand-how-the-ubi-module-content-was-pasted-into-the-new-iascable-gitops-ubi-repository","text":"Following the concept for the gitops bootstrap setup documented in the template-terraform-gitops GitHub repository. We have two main folders in the iascable-gitops-ubi repository. One for the Argo CD application configurations called argocd One for the application which will be deployed be the Argo CD application configurations called payload. Let us inspect these two folders. The gif below shows some of the created files and folders.","title":"7.1 Understand how the ubi module content was pasted into the new iascable-gitops-ubi repository"},{"location":"reference/glossary/","text":"Glossary \u00b6 A \u00b6 Term Description Ascent The project name for the builder UI B \u00b6 Term Description Bill of Materials Recipe for creating a desired environment BOM Acronym of Bill of Materials I \u00b6 Term Description iascable The command line tool to generate the automation assets from a Bill of Materials M \u00b6 Term Description Module A composable block to setup/install a part of an environment Module catalog The online catalog of available modules","title":"Glossary"},{"location":"reference/glossary/#glossary","text":"","title":"Glossary"},{"location":"reference/glossary/#a","text":"Term Description Ascent The project name for the builder UI","title":"A"},{"location":"reference/glossary/#b","text":"Term Description Bill of Materials Recipe for creating a desired environment BOM Acronym of Bill of Materials","title":"B"},{"location":"reference/glossary/#i","text":"Term Description iascable The command line tool to generate the automation assets from a Bill of Materials","title":"I"},{"location":"reference/glossary/#m","text":"Term Description Module A composable block to setup/install a part of an environment Module catalog The online catalog of available modules","title":"M"},{"location":"reference/overview/","text":"Reference \u00b6 Todo Complete this section Reference material for Module directory template content Module file syntax BOM syntax CLI parameters (iascable) Links to useful resources","title":"Overview"},{"location":"reference/overview/#reference","text":"Todo Complete this section Reference material for Module directory template content Module file syntax BOM syntax CLI parameters (iascable) Links to useful resources","title":"Reference"},{"location":"resources/overview/","text":"Resources \u00b6 ASCENT Automation bundles Automation runtimes Module catalog Troubleshooting","title":"Overview"},{"location":"resources/overview/#resources","text":"ASCENT Automation bundles Automation runtimes Module catalog Troubleshooting","title":"Resources"},{"location":"resources/ascent/","text":"ASCENT \u00b6","title":"ASCENT"},{"location":"resources/ascent/#ascent","text":"","title":"ASCENT"},{"location":"resources/automation-bundles/","text":"Automation bundles \u00b6","title":"Automation bundles"},{"location":"resources/automation-bundles/#automation-bundles","text":"","title":"Automation bundles"},{"location":"resources/automation-runtimes/","text":"Automation runtime environments \u00b6 Supported runtimes \u00b6 There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled. Docker Desktop \u00b6 Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Multipass \u00b6 Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation. Unsupported runtimes \u00b6 Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed Colima instructions \u00b6 Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation Podman instructions \u00b6 Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode. Getting started with Podman for MacOS Install Brew Install Podman (a replacement for Docker Desktop ) and the docker cli brew install podman docker Create a podman machine, set it to run in rootful mode and start it podman machine init podman machine set --rootful podman machine start Once the podman vm is started, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Dealing with known issues for Podman on MacOS When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module. Getting started with Podman for Linux Visit and follow the installation instructions for your distribution Once the podman application is installed provide sudo podman as the first argument to the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed: ./launch.sh 'sudo podman' More information available at: https://podman.io/getting-started/installation","title":"Automation Runtimes"},{"location":"resources/automation-runtimes/#automation-runtime-environments","text":"","title":"Automation runtime environments"},{"location":"resources/automation-runtimes/#supported-runtimes","text":"There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled.","title":"Supported runtimes"},{"location":"resources/automation-runtimes/#docker-desktop","text":"Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed.","title":"Docker Desktop"},{"location":"resources/automation-runtimes/#multipass","text":"Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation.","title":"Multipass"},{"location":"resources/automation-runtimes/#unsupported-runtimes","text":"Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed","title":"Unsupported runtimes"},{"location":"resources/automation-runtimes/#colima-instructions","text":"Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation","title":"Colima instructions"},{"location":"resources/automation-runtimes/#podman-instructions","text":"Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode.","title":"Podman instructions"},{"location":"resources/module-catalog/","text":"Module catalog \u00b6","title":"Module catalog"},{"location":"resources/module-catalog/#module-catalog","text":"","title":"Module catalog"},{"location":"resources/troubleshooting/","text":"Troubleshooting \u00b6 Uninstalling \u00b6 To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in the README file in the repository Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve Variables may not be used here. \u00b6 You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct Intermittent network failures when using Colima \u00b6 If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures. Resources stuck in Terminating state \u00b6 When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications Workspace permission issues \u00b6 Root user on Linux \u00b6 If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions. Legacy launch.sh script \u00b6 IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README in the repository to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration. That didn't work, what next? \u00b6 If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"Troubleshooting"},{"location":"resources/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"resources/troubleshooting/#uninstalling","text":"To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in the README file in the repository Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve","title":"Uninstalling"},{"location":"resources/troubleshooting/#variables-may-not-be-used-here","text":"You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct","title":"Variables may not be used here."},{"location":"resources/troubleshooting/#intermittent-network-failures-when-using-colima","text":"If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures.","title":"Intermittent network failures when using Colima"},{"location":"resources/troubleshooting/#resources-stuck-in-terminating-state","text":"When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications","title":"Resources stuck in Terminating state"},{"location":"resources/troubleshooting/#workspace-permission-issues","text":"","title":"Workspace permission issues"},{"location":"resources/troubleshooting/#root-user-on-linux","text":"If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions.","title":"Root user on Linux"},{"location":"resources/troubleshooting/#legacy-launchsh-script","text":"IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README in the repository to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration.","title":"Legacy launch.sh script"},{"location":"resources/troubleshooting/#that-didnt-work-what-next","text":"If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"That didn't work, what next?"},{"location":"tasks/overview/","text":"Tasks \u00b6 Todo Complete this section outline the tasks needed to be able to use a provided Bill of Materials or build and deploy a custom Bill of Materials using the builder UI (ascent) outline the tasks needed to be able to build and deploy a Bill of Materials using the iascable CLI tool outline the tasks needed to extend the toolkit by creating, testing and publishing additional modules","title":"Overview"},{"location":"tasks/overview/#tasks","text":"Todo Complete this section outline the tasks needed to be able to use a provided Bill of Materials or build and deploy a custom Bill of Materials using the builder UI (ascent) outline the tasks needed to be able to build and deploy a Bill of Materials using the iascable CLI tool outline the tasks needed to extend the toolkit by creating, testing and publishing additional modules","title":"Tasks"},{"location":"tutorials/1-setup/","text":"Setup you environment \u00b6 This tutorial will prepare your workstation or laptop to be able to deploy a Bill of Material or create a new module to extend the catalog of available modules. Installing the environment \u00b6 Note the curl utility is used in some of the instructions in this tutorial. Most OS installations come with curl preinstalled, if not you should install curl before proceeding. Select which environment you want to work with. There is a discussion about these options in the Getting Started section. Docker Docker Desktop can be installed directly from the Docker website Multipass Multipass can be installed from the Multipass website MacOS users After installation MacOS users need to give Multipass permission to access their local hard disk: Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . Once installed you need to prepare a virtual machine with the necessary tools installed. To do this you need to: Open a command line window (on Windows you should work in a Windows Subsystem for Linux shell). Navigate to a local directory you want to work in, using the cd command Download the virtual machine initialization file using command: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Launch the Multipass virtual machine with command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine mount your local filesystem for use within the virtual machine with the following command: multipass mount $PWD cli-tools:/automation unsupported-Podman Warning This is an unsupported environment. You may encounter issues using this environment and the developers may choose not to fix any issues identified To install Podman please follow instructions for your OS on the podman website On Mac and Windows you need to enable rootful mode, so you need to set this before running the machine start command: podman machine init podman machine set --rootful podman machine start unsupported-Colima Warning This is an unsupported environment. You may encounter issues using this environment and the developers may choose not to fix any issues identified Colima can be installed using one of the following package management tools. Homebrew , Macports or Nix Homebrew brew install colima docker MacPorts sudo port install colima Nix nix-env -iA nixpkgs.colima Additional installation can be found on the colima github repository Install local tools \u00b6 Install the following command line tools on your workstation or laptop: oc - The OpenShift command line utility iascable - the Toolkit command line utility to work with Bill of Materials Install oc \u00b6 Follow the instructions in the OpenShift documentation to install the oc cli. Install iascable \u00b6 Iascable is a command line tool to work with Bill of Material files. It can be installed or updated by running the following command: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Setup"},{"location":"tutorials/1-setup/#setup-you-environment","text":"This tutorial will prepare your workstation or laptop to be able to deploy a Bill of Material or create a new module to extend the catalog of available modules.","title":"Setup you environment"},{"location":"tutorials/1-setup/#installing-the-environment","text":"Note the curl utility is used in some of the instructions in this tutorial. Most OS installations come with curl preinstalled, if not you should install curl before proceeding. Select which environment you want to work with. There is a discussion about these options in the Getting Started section. Docker Docker Desktop can be installed directly from the Docker website Multipass Multipass can be installed from the Multipass website MacOS users After installation MacOS users need to give Multipass permission to access their local hard disk: Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . Once installed you need to prepare a virtual machine with the necessary tools installed. To do this you need to: Open a command line window (on Windows you should work in a Windows Subsystem for Linux shell). Navigate to a local directory you want to work in, using the cd command Download the virtual machine initialization file using command: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Launch the Multipass virtual machine with command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine mount your local filesystem for use within the virtual machine with the following command: multipass mount $PWD cli-tools:/automation unsupported-Podman Warning This is an unsupported environment. You may encounter issues using this environment and the developers may choose not to fix any issues identified To install Podman please follow instructions for your OS on the podman website On Mac and Windows you need to enable rootful mode, so you need to set this before running the machine start command: podman machine init podman machine set --rootful podman machine start unsupported-Colima Warning This is an unsupported environment. You may encounter issues using this environment and the developers may choose not to fix any issues identified Colima can be installed using one of the following package management tools. Homebrew , Macports or Nix Homebrew brew install colima docker MacPorts sudo port install colima Nix nix-env -iA nixpkgs.colima Additional installation can be found on the colima github repository","title":"Installing the environment"},{"location":"tutorials/1-setup/#install-local-tools","text":"Install the following command line tools on your workstation or laptop: oc - The OpenShift command line utility iascable - the Toolkit command line utility to work with Bill of Materials","title":"Install local tools"},{"location":"tutorials/1-setup/#install-oc","text":"Follow the instructions in the OpenShift documentation to install the oc cli.","title":"Install oc"},{"location":"tutorials/1-setup/#install-iascable","text":"Iascable is a command line tool to work with Bill of Material files. It can be installed or updated by running the following command: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Install iascable"},{"location":"tutorials/overview/","text":"Tutorials \u00b6 Todo Complete this section Provide a learning journey to go from setup and 'Hello World' getting started experience to production ready use of the tooling: using builder UI (ascent) using iascable CLI for more advanced use cases creating new modules to extend the toolkit contributing back to community hosting private catalog of modules","title":"Overview"},{"location":"tutorials/overview/#tutorials","text":"Todo Complete this section Provide a learning journey to go from setup and 'Hello World' getting started experience to production ready use of the tooling: using builder UI (ascent) using iascable CLI for more advanced use cases creating new modules to extend the toolkit contributing back to community hosting private catalog of modules","title":"Tutorials"}]}